import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torch.optim as optim
import torchvision.datasets as datasets
import imageio
import numpy as np
import matplotlib
from torchvision.utils import make_grid, save_image
from torch.utils.data import DataLoader
from matplotlib import pyplot as plt
from tqdm import tqdm
matplotlib.style.use('ggplot')

class Generator(nn.Module):
    def __init__(self, nz):
        super(Generator, self).__init__()
        self.nz = nz # the dimension of the random noise used to seed the Generator
        self.main = nn.Sequential( # nn.sequential is a handy way of combining multiple layers.
            nn.Linear(self.nz, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, 784),
            nn.Tanh(),
        )
    def forward(self, x):
        return self.main(x).view(-1, 1, 28, 28)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.n_input = 784
        self.main = nn.Sequential(
            nn.Linear(self.n_input, 1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        )
    def forward(self, x):
        x = x.view(-1, 784)
        return self.main(x)

def train_discriminator(optimizer, real_data, fake_data):
    """
    Train the discriminator on a minibatch of data.
    INPUTS
        :param optimizer: the optimizer used for training
        :param real_data: the batch of training data
        :param fake_data: the data generated by the generator from random noise
    The discriminator will incur two losses: one from trying to classify the real data, and another from classifying the fake data.
    TODO: Fill in this function.
    It should
    1. Run the discriminator on the real_data and the fake_data
    2. Compute and sum the respective loss terms (described in the assignment)
    3. Backpropogate the loss (e.g. loss.backward()), and perform optimization (e.g. optimizer.step()).
    """
    batch_size = real_data.shape[0]
    
    # zero out the gradients
    optimizer.zero_grad()

    # feed real data into discriminator to get output
    real_out = discriminator(real_data)

    # feed fake data into discriminator to get output
    fake_out = discriminator(fake_data) 

    # generate labels of real data (1)
    real_label = torch.ones((batch_size, ), device=device).reshape(real_out.shape)

    # generate labels of fake data (0)
    fake_label = torch.zeros((batch_size, ), device=device).reshape(fake_out.shape)


    # compute discriminator loss
    loss = 0.5 * (criterion(real_out, real_label) + criterion(fake_out, fake_label))

    # Backpropogate the loss
    loss.backward()

    # perform a step of optimization
    optimizer.step()

    # we'll return the loss for book-keeping purposes. (E.g. if you want to make plots of the loss.)
    return loss


def train_generator(optimizer, fake_data):
    """
    Performs a single training step on the generator.
    :param optimizer: the optimizer
    :param fake_data: forgeries, created by the generator from random noise. (Done before calling this function.)
    :return:  the generator's loss
    TODO: Fill in this function
    It should
    1. Run the discriminator on the fake_data
    2. compute the resultant loss for the generator (as described in the assignment)
    3. Backpropagate the loss, and perform optimization
    """
    batch_size = fake_data.shape[0]
 
    # zero out the gradients
    optimizer.zero_grad()

    # feed fake data into discriminator to get output
    fake_out = discriminator(fake_data)

    # generate labels of fake data (1)
    fake_label = torch.ones((batch_size, ), device=device).reshape(fake_out.shape)
   

    # compute generator loss
    loss = 0.5* criterion(fake_out, fake_label)

    # Backpropagate the loss
    loss.backward()

    # perform a step of optimization
    optimizer.step()

    return loss



# import data
batch_size = 100
train_data = datasets.MNIST(
    root='../data',
    train=True,
    download=True,
    transform=transforms.ToTensor()
)
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)


# initialize geneartor and discriminator
nz = 25 # dimension of random noise
generator = Generator(nz).to(device)
discriminator = Discriminator().to(device)

# initialize binary cross entropy loss 
criterion = nn.BCELoss(reduction='mean')

# initialize Adam optimizer
lr_D, lr_G = 1e-4, 1e-4 # a higher learning rate for generator
g_optimizer = optim.Adam(generator.parameters(), lr_G, betas=(0.5, 0.999)) 
d_optimizer = optim.Adam(discriminator.parameters(), lr_D, betas=(0.5, 0.999))
g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=5, gamma=0.1)
d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=5, gamma=0.1)

#TODO: Build a training loop for the GAN
# For each epoch, you'll
# 1. Loop through the training data. For each batch, feed random noise into the generator to generate fake_data of the corresponding size.
# 2. Feed the fake data and real data into the train_discriminator and train_generator functions
# At the end of each epoch, use the below functions to save a grid of generated images.
d_epoch_loss, g_epoch_loss = np.zeros(num_epochs), np.zeros(num_epochs)

for epoch in range(num_epochs):
    d_batch_loss, g_batch_loss = np.zeros(len(train_loader)), np.zeros(len(train_loader))
    for data, _ in train_loader:
        # perform training
        data = data.to(device)

        # generate random noise
        noise = torch.normal(0, 1, size=(batch_size, nz)).to(device)

        # feed random noise into the generator to generate fake data
        fake_data = generator(noise)

        # first update the discriminator and then the generator
        # train discriminator, Do not need to compute gradient for generator, detach it from computing gradients.
        d_batch_loss[i] = train_discriminator(d_optimizer, data, fake_data.detach()).item()

        # generate new random noise
        noise2 = torch.normal(0, 1, size=(batch_size, nz)).to(device)

        # feed random noise into the generator to generate fake data
        fake_data = generator(noise2)

        # train generator
        g_batch_loss[i] = train_generator(g_optimizer, fake_data).item()
      
    d_epoch_loss[epoch] = np.mean(d_batch_loss)
    g_epoch_loss[epoch] = np.mean(g_batch_loss)

    d_scheduler.step()
    g_scheduler.step() 

    if epoch % 5 == 0:
      print("Epoch: {} Generator loss: {:.4f}\t Discriminator loss: {:.4f}".format(epoch, g_epoch_loss[epoch], d_epoch_loss[epoch]))

    # generate new random noise
    noise = torch.normal(0, 1, size=(10, nz)).to(device)

    # generate images
    generated_img = generator(noise).detach()

    generated_img = make_grid(generated_img, nrow=10)

    # show(generated_img)  

plt.plot(range(num_epochs), d_epoch_loss, 'g', label="Discriminator loss")
plt.plot(range(num_epochs), g_epoch_loss, 'b', label="Generator loss")
plt.title("GAN: Discriminator loss and Generator loss")
plt.xlabel("Number of iterations")
plt.ylabel("Loss")
plt.legend()
plt.savefig('GAN_generator_discriminator_loss.png') 
plt.show()
