
import matplotlib.pyplot as plt
import numpy as np
import os.path
import pickle
import time
from contextlib import contextmanager
from typing import Callable, Generator, Optional
import torch
import torchvision.transforms.functional as F
from torchvision.utils import save_image, make_grid
"""
Utility functions. DO NOT EDIT THIS FILE.
"""


def show(imgs):
    """plt grid image"""
    if not isinstance(imgs, list):
        imgs = [imgs]
    
    plt.rcParams["savefig.bbox"] = 'tight'
    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)
    for i, img in enumerate(imgs):
        img = F.to_pil_image(img)
        axs[0, i].imshow(np.asarray(img))
        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])


def interpolate(embeddings, labels, model, n=10, label1=0, label2=0):
    """
    generate `n` fabricated embeddings by interpolating between embeddings of 2 distinct images

    @params:
    embeddings: embeddings of images generated by Autoencoder or Variational Autoencoder 
    n: number of fabricated embeddings generated, default to 10 
    label1: label of first image class, default to 0 
    label2: label of second image class, default to 0
    """
   

    # select the first image of label 1
    img1 = embeddings[labels == label1][0]

    if label1 == label2:
      
      # select the 25th image of label 1
      img2 = embeddings[labels == label1][25] 

    else:

      # select the first image of label 2
      img2 = embeddings[labels == label2][0] 

    # concatenate 2 images to a numpy array 2 x 2
    imgs = np.stack((img1, img2)) 

    # interpolate `n` fabricated embeddings
    fabricated_embeddings = np.zeros((n, embeddings.shape[1])) 
    for i in range(n):
        # generate weight matrix 1 x `embeddings.shape[1]`
        w1 = np.random.uniform()
        weight = np.stack((w1, 1-w1))
        fabricated_embeddings[i] = weight @ imgs

    fabraicated_imgs = model.decode(torch.Tensor(fabricated_embeddings).to(device)) 

    # reshape the image tensors into a grid
    fabraicated_imgs = make_grid(fabraicated_imgs.view(-1, 1, 28, 28).detach())

    show(fabraicated_imgs)

    # save the generated torch tensor images
    save_image(fabraicated_imgs, "fabraicated_imgs.png")

    return img1, img2, fabricated_embeddings

    # fabraicated_imgs = imageio.imread("/content/fabraicated_imgs.png")

    # show(fabraicated_imgs)


def plot_inter_embed(embeddings, img1, img2, inter_embed, classes="between"):

    # plot 2D Embeddings of 1000 digits, colored by image labels
    fig, ax = plt.subplots()
    plt.xlabel("$x_1$")
    plt.ylabel("$x_2$")

    scatter = ax.scatter(embeddings[:, 0], embeddings[:, 1],  c=labels, s=1)
    legend = ax.legend(*scatter.legend_elements(),
                    loc="best", title="Class",
                    bbox_to_anchor=(1, 1))

    # plot 2D original embeddings
    scatter = ax.scatter(img1[0], img1[1],  c='r')
    scatter = ax.scatter(img2[0], img2[1],  c='r')

    # plot 2D interpolated embeddings
    scatter = ax.scatter(inter_embed[:, 0], inter_embed[:,1],  c='b', s=12)

    ax.add_artist(legend)

    if classes == "between":
      plt.title(f"Autoencoder: Fabricated 2D Embeddings between digit {label1} and digit {label2}")

    elif classes == "within":
      plt.title(f"Autoencoder: Fabricated 2D Embeddings within digits {label1}")

    plt.show()

def cache_pickle(func: Callable) -> Callable:
    """
    A decorator that caches the output of a function to a pickle file.
    It adds a keyword parameter called "cache_filename" that gives the
    name of the cached file.
    """

    def _func(*func_args, cache_filename: Optional[str] = None, **func_kwargs):
        if cache_filename is not None and os.path.isfile(cache_filename):
            message = "Loading cached function output from {}..." \
                      "".format(cache_filename)
            with timer(message):
                with open(cache_filename, "rb") as f:
                    return pickle.load(f)

        return_val = func(*func_args, **func_kwargs)

        if cache_filename is not None:
            message = "Saving function output to {}..." \
                      "".format(cache_filename)
            with timer(message):
                with open(cache_filename, "wb") as f:
                    pickle.dump(return_val, f)

        return return_val

    return _func


@contextmanager
def timer(message: str) -> Generator[Callable[[None], float], None, None]:
    """
    A timer that measures the time spent running code within a with-
    block.

    :param message: A message to be printed when the timer starts.
    :return: A generator that yields a function that returns the current
        time elapsed.
    """
    print(message)

    # Start timer
    start_time = time.time()
    yield lambda: time.time() - start_time

    # Stop timer
    end_time = time.time()
    elapsed = end_time - start_time
    print("Done. Time elapsed: {:.3f} seconds".format(elapsed))